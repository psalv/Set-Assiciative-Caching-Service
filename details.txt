

n-Way Set Associative Cache


Requirements:
1) Cache itself is entirely within memory (does not communicate with backing store).
2) Client interface should be type-safe for keys and values and allow for both the keys and values to be of
an arbitrary type (strings, integers, classes, ...).

3) Design the interface as a library to be distributed to clients, assume the client doesn't have the source
code of your library.
    https://stackoverflow.com/questions/15746675/how-to-write-a-python-module-package?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
    - make installable with pip

4) Provide LRU and MRU replacement algorithms.
5) Provide a way for any alternative replacement algorithms to be implemented by the client and used by the cache.
    - set as a parameter upon class creation, saving into a self variable (either "LRU", "MRU", otherFn)

Example use case:
As an in-memory cache on an application server, storing data associated with user id,
in order to avoid a database dip for every request.



Design Requirements:
Provide a PDF describing your design. The goal is to communicate the most important aspects of your solution
to other developers; in particular, the key things you think we should know to aid us in our evaluation of your solution.




Vlad notes:

Need multiple readers and multiple writers
Manager class takes in requests from a FIFO queue
    - nonblocking concurrent queue, guava has such a queue
zlib to save memory for caching
To identify priority have an enum that is checked periodically










Notes on caches:

Caches work by storing data that is likely to be used.

A Cache on the other hand is generally configured to evict entries automatically,
in order to constrain its memory footprint.

Unit of storage within a cache is the line, which contains several consecutive 'words',
several consecutive items can be read from the cache without a miss.

Each data item is identified by a key and retrieved by it's key.
key = current address from the processor
data = data stored at the location

associative memory does not contain ordered elements



In an n-ay set-associated cache, there are n possible cache locations that a given line can be loaded to.
(typically n is in the range 2 to 8)

Each of the direct map caches are searched in **parallel** and feed into an or gate determining if a hit has occurred.


A memory block maps to a unique set (specified by the index field), can be placed in any way within that set
    (block address) modulo (# sets in the cache)



** I do not believe I am building a low level OS cache, but rather an application cache (as indicated in the use-case)
Requirements of this cache:
- store items
    - remove unused items by either LRU, MRU, or custom method
    - need to by n-ways set associative, which will work just like the blocks, however we are storing objects
    instead of blocks
- get items












**** READ THESE NOTES ****

https://www.youtube.com/watch?v=ReKzEGLlGfk

Direct mapping:
- an entry has only 1 line that it can map to in the cache

Associative mapping:
- an entry can map to any line in the cache
- requires a lookup table (matching the key to the location in the cache)

set associative:
- slots of the cache memory are divided into sets
- each set has the same structure: tag, data, validity bit
- only 1 of the sets is determined uniquely (think performing a hash % n)
- within the set, we follow an associative mapping (still require a lookup)
- *** 1 item => only 1 set, but can go to any row within that set ***

data = # sets * lines/set * size/line

- simpler lookup (only check the tags within 1 of the sets instead of them all)
- can still use all of the lines within a particular set
    - *** note we aren't waiting for the entire cache to fill up to remove, but rather the particular set







Design:

    A set is represented by a map
    We have n sets.
    Each of the n sets can have l lines (objects) within them.
    Objects map to a particular line; however can be in any set.

    *** PROBLEM *** How do we store in a particular set without storing in them all???
        - Solution: We need some thread safe method, that blocks other threads when currently being used,
        this method will be the one to write into the set and will finish by communicating with the other
        flags in order to inform them that the action has been finished

        *** Not the method is safe but just the section that involves altering the sets,
        this way they can check for lru or mru in parallel.
            > using a mutex for the intro but then ensuring all 4 are done to not store potentially twice

    *** PROBLEM *** How do we know which line it maps to???
        - Solution: it will always map to the same line based on it's hash

    *** PROBLEM *** What if the line it maps to is full/how do we fill the entire map???
        - Possible solution: the maps are direct maps, meaning that we fill in entries in the order in which they come,
        so we do not need to worry about which line they map to but rather, we need to worry about the sizes of the maps
            - This seems dumb, why not use just one big map???

        - There are n possible cache locations that a data entry can be loaded into
            - within each set there is only 1 possible
        - Hash of item is fed into each direct mapped cache in parallel

    When we check if an element exist we must check every set concurrently.
    If it does exist we return the associated data for the key, otherwise we return some form of error flag
    When we add to an item we need to check if the cache is full, and if so employ our replacement algorithm to remove
    an entry.



    Things I need to keep track of:

        keys
        data
        data location
        when data was last accessed


    Have n threads

    When something is added to either fifo queue, the threads all awaken.

    Add:
        First check if the data exists






Current Tasks:

2) Decide how to store data
    - should i store it with the key
    - what about direct mapping


